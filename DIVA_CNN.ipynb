{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 960 (CNMeM is enabled with initial size: 70.0% of memory, cuDNN 4007)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (60000, 1, 28, 28))\n",
      "(60000, 'train samples')\n",
      "(10000, 'test samples')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compares convolutional MLPs to convolutional DIVA.\n",
    "Based on the code from the Keras CNN example.\n",
    "\n",
    "With the major caveat that I've only run this twice, due to time constraints it appears\n",
    "that convolutional DIVA takes forever to converge, and runtime per epoch is also quite long.\n",
    "Increasing the epochs(12 to 50) and the hidden units(128 to 512) doesn't appear to \n",
    "help much, though it still gets decent (~82%) accuracy even when loss is pretty high.\n",
    "It is not known if DIVA doesn't work well with convolution or if SGD is insufficient.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "nb_pool = 2\n",
    "# convolution kernel size\n",
    "nb_conv = 3\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#for DIVA output\n",
    "alt_X_train = X_train.reshape(60000, 784)\n",
    "alt_X_test = X_test.reshape(10000, 784)\n",
    "\n",
    "#for convolutional layer\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 6s - loss: 0.3674 - acc: 0.8857 - val_loss: 0.0923 - val_acc: 0.9705\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 6s - loss: 0.1485 - acc: 0.9556 - val_loss: 0.0604 - val_acc: 0.9809\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 6s - loss: 0.1133 - acc: 0.9653 - val_loss: 0.0504 - val_acc: 0.9847\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 6s - loss: 0.0917 - acc: 0.9732 - val_loss: 0.0446 - val_acc: 0.9846\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 6s - loss: 0.0776 - acc: 0.9772 - val_loss: 0.0394 - val_acc: 0.9873\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 6s - loss: 0.0696 - acc: 0.9787 - val_loss: 0.0365 - val_acc: 0.9875\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 6s - loss: 0.0629 - acc: 0.9804 - val_loss: 0.0362 - val_acc: 0.9874\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 6s - loss: 0.0604 - acc: 0.9824 - val_loss: 0.0346 - val_acc: 0.9888\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 6s - loss: 0.0542 - acc: 0.9837 - val_loss: 0.0312 - val_acc: 0.9890\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 6s - loss: 0.0501 - acc: 0.9849 - val_loss: 0.0310 - val_acc: 0.9898\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 6s - loss: 0.0483 - acc: 0.9854 - val_loss: 0.0296 - val_acc: 0.9899\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 6s - loss: 0.0439 - acc: 0.9868 - val_loss: 0.0294 - val_acc: 0.9900\n",
      "('Test score:', 0.029353955537044386)\n",
      "('Test accuracy:', 0.98999999999999999)\n"
     ]
    }
   ],
   "source": [
    "#MLP\n",
    "nb_epoch = 12\n",
    "batch_size = 128\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                        border_mode='valid',\n",
    "                        input_shape=(1, img_rows, img_cols)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DIVA.py:71: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if(alt_X_train!=None):\n",
      "DIVA.py:82: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if(alt_X_train==None):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss 404192037.927734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DIVA.py:91: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if(alt_X_test==None):\n",
      "DIVA.py:121: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if(alt_X_test==None):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.701700\n",
      "Epoch 2\n",
      "Loss 353784281.215454\n",
      "Test Accuracy: 0.722000\n",
      "Epoch 3\n",
      "Loss 315463434.567505\n",
      "Test Accuracy: 0.739600\n",
      "Epoch 4\n",
      "Loss 287233212.600525\n",
      "Test Accuracy: 0.755300\n",
      "Epoch 5\n",
      "Loss 266432154.286255\n",
      "Test Accuracy: 0.772500\n",
      "Epoch 6\n",
      "Loss 251101931.762024\n",
      "Test Accuracy: 0.784300\n",
      "Epoch 7\n",
      "Loss 239801295.748047\n",
      "Test Accuracy: 0.792700\n",
      "Epoch 8\n",
      "Loss 231469294.718750\n",
      "Test Accuracy: 0.799800\n",
      "Epoch 9\n",
      "Loss 225324791.851929\n",
      "Test Accuracy: 0.805000\n",
      "Epoch 10\n",
      "Loss 220792517.384766\n",
      "Test Accuracy: 0.808500\n",
      "Epoch 11\n",
      "Loss 217448746.554382\n",
      "Test Accuracy: 0.812800\n",
      "Epoch 12\n",
      "Loss 214981286.732910\n",
      "Test Accuracy: 0.814300\n",
      "Epoch 13\n",
      "Loss 213160097.942871\n",
      "Test Accuracy: 0.816400\n",
      "Epoch 14\n",
      "Loss 211815614.165161\n",
      "Test Accuracy: 0.816400\n",
      "Epoch 15\n",
      "Loss 210822846.067383\n",
      "Test Accuracy: 0.817700\n",
      "Epoch 16\n",
      "Loss 210089594.004272\n",
      "Test Accuracy: 0.818600\n",
      "Epoch 17\n",
      "Loss 209547911.228821\n",
      "Test Accuracy: 0.819400\n",
      "Epoch 18\n",
      "Loss 209147664.533508\n",
      "Test Accuracy: 0.819000\n",
      "Epoch 19\n",
      "Loss 208851844.926880\n",
      "Test Accuracy: 0.819500\n",
      "Epoch 20\n",
      "Loss 208633135.711975\n",
      "Test Accuracy: 0.819500\n",
      "Epoch 21\n",
      "Loss 208471411.272400\n",
      "Test Accuracy: 0.819400\n",
      "Epoch 22\n",
      "Loss 208351787.127747\n",
      "Test Accuracy: 0.820100\n",
      "Epoch 23\n",
      "Loss 208263273.148743\n",
      "Test Accuracy: 0.820800\n",
      "Epoch 24\n",
      "Loss 208197768.090759\n",
      "Test Accuracy: 0.821000\n",
      "Epoch 25\n",
      "Loss 208149267.761597\n",
      "Test Accuracy: 0.821200\n",
      "Epoch 26\n",
      "Loss 208113346.310730\n",
      "Test Accuracy: 0.821100\n",
      "Epoch 27\n",
      "Loss 208086728.763733\n",
      "Test Accuracy: 0.821200\n",
      "Epoch 28\n",
      "Loss 208066997.882263\n",
      "Test Accuracy: 0.820800\n",
      "Epoch 29\n",
      "Loss 208052372.360535\n",
      "Test Accuracy: 0.820800\n",
      "Epoch 30\n",
      "Loss 208041508.573059\n",
      "Test Accuracy: 0.820600\n",
      "Epoch 31\n",
      "Loss 208033468.226868\n",
      "Test Accuracy: 0.820600\n",
      "Epoch 32\n",
      "Loss 208027474.404968\n",
      "Test Accuracy: 0.820500\n",
      "Epoch 33\n",
      "Loss 208023027.485901\n",
      "Test Accuracy: 0.820200\n",
      "Epoch 34\n",
      "Loss 208019729.219482\n",
      "Test Accuracy: 0.820400\n",
      "Epoch 35\n",
      "Loss 208017269.435791\n",
      "Test Accuracy: 0.820400\n",
      "Epoch 36\n",
      "Loss 208015430.017334\n",
      "Test Accuracy: 0.820400\n",
      "Epoch 37\n",
      "Loss 208014057.779541\n",
      "Test Accuracy: 0.820300\n",
      "Epoch 38\n",
      "Loss 208013038.072571\n",
      "Test Accuracy: 0.820100\n",
      "Epoch 39\n",
      "Loss 208012278.490295\n",
      "Test Accuracy: 0.820100\n",
      "Epoch 40\n",
      "Loss 208011711.636292\n",
      "Test Accuracy: 0.820100\n",
      "Epoch 41\n",
      "Loss 208011285.941345\n",
      "Test Accuracy: 0.820200\n",
      "Epoch 42\n",
      "Loss 208010963.011841\n",
      "Test Accuracy: 0.820300\n",
      "Epoch 43\n",
      "Loss 208010721.181213\n",
      "Test Accuracy: 0.820400\n",
      "Epoch 44\n",
      "Loss 208010539.061279\n",
      "Test Accuracy: 0.820500\n",
      "Epoch 45\n",
      "Loss 208010401.588135\n",
      "Test Accuracy: 0.820600\n",
      "Epoch 46\n",
      "Loss 208010296.638062\n",
      "Test Accuracy: 0.820600\n",
      "Epoch 47\n",
      "Loss 208010216.657227\n",
      "Test Accuracy: 0.820500\n",
      "Epoch 48\n",
      "Loss 208010155.578125\n",
      "Test Accuracy: 0.820500\n",
      "Epoch 49\n",
      "Loss 208010109.085266\n",
      "Test Accuracy: 0.820500\n",
      "Epoch 50\n",
      "Loss 208010073.247070\n",
      "Test Accuracy: 0.820500\n",
      "Done training\n",
      "Test Accuracy: 0.820500\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import DIVA\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\"\"\"\n",
    "Py-DIVA can set up DIVA at the end of an existing model. \n",
    "I don't want to diverge too much from base Keras, \n",
    "so this is intentionally the only way to use anything \n",
    "beyond a standard DIVA, including deep DIVA or anything \n",
    "with convolution, max pooling, or dropout. \n",
    "\"\"\"\n",
    "\n",
    "#Set up convolutional layer\n",
    "premodel = Sequential()\n",
    "\n",
    "premodel.add(Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                        border_mode='valid',\n",
    "                        input_shape=(1, img_rows, img_cols)))\n",
    "premodel.add(Activation('relu'))\n",
    "premodel.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "premodel.add(Activation('relu'))\n",
    "premodel.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "premodel.add(Dropout(0.25))\n",
    "premodel.add(Flatten())\n",
    "\n",
    "input_shape=784\n",
    "num_hidden=512\n",
    "nb_epoch=50\n",
    "\n",
    "\n",
    "#compile model\n",
    "diva_model = DIVA.diva(nb_classes, input_shape, num_hidden, \n",
    "                       hidden_act='relu', loss='mean_squared_error', \n",
    "                       optimizer=SGD(), prev_model=premodel,\n",
    "                       compare=DIVA.compareMSE)\n",
    "\n",
    "print('Training Model')\n",
    "\n",
    "#train model\n",
    "train_metrics=diva_model.train(X_train, y_train, nb_epoch, 1, X_test, y_test, \n",
    "                               alt_X_train=alt_X_train, alt_X_test=alt_X_test)            \n",
    "            \n",
    "#test model\n",
    "accuracy=diva_model.test(X_test, y_test, 1, alt_X_test=alt_X_test) \n",
    "\n",
    "print('Done')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
