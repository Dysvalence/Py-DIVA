{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 960 (CNMeM is enabled with initial size: 70.0% of memory, cuDNN 4007)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compares slightly deep (2 hidden layer) MLP to DIVA.\n",
    "Based on the Keras MLP example.\n",
    "\"\"\"\n",
    "import DIVA\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "nb_classes = 10\n",
    "nb_epoch = 50\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                       Output Shape        Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                    (None, 512)         401920      dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)          (None, 512)         0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)                (None, 512)         0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                    (None, 512)         262656      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)          (None, 512)         0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)                (None, 512)         0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                    (None, 10)          5130        dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)          (None, 10)          0           dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 669706\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.259194416425\n",
      "Test accuracy: 0.9802\n"
     ]
    }
   ],
   "source": [
    "#Testing Deep MLP\n",
    "batch_size=20\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=0, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                       Output Shape        Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_4 (Dense)                    (None, 512)         401920      dense_input_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)          (None, 512)         0           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)                (None, 512)         0           activation_4[1][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                    (None, 512)         262656      dropout_3[1][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)          (None, 512)         0           dense_5[1][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                    (None, 784)         402192      activation_5[1][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)          (None, 784)         0           dense_6[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 1066768\n",
      "____________________________________________________________________________________________________\n",
      "Epoch 1\n",
      "Loss 4690.338523\n",
      "Test Accuracy: 0.844800\n",
      "Epoch 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Py-DIVA can set up DIVA at the end of an existing model. \n",
    "I don't want to diverge too much from base Keras, \n",
    "so this is intentionally the only way to run deep DIVAs. \n",
    "\"\"\"\n",
    "\n",
    "premodel = Sequential()\n",
    "premodel.add(Dense(512, input_shape=(784,)))\n",
    "premodel.add(Activation('relu'))\n",
    "premodel.add(Dropout(0.2))\n",
    "premodel.add(Dense(512))\n",
    "premodel.add(Activation('relu'))\n",
    "\n",
    "num_hidden=512\n",
    "input_shape=784\n",
    "\n",
    "#compile model\n",
    "diva_model = DIVA.diva(nb_classes, input_shape, num_hidden, \n",
    "                       hidden_act='relu', loss='mean_squared_error', \n",
    "                       optimizer=SGD(), prev_model=premodel,\n",
    "                       compare=DIVA.compareMSE)\n",
    "\n",
    "diva_model.channels[0].summary()\n",
    "\n",
    "#train model\n",
    "train_metrics=diva_model.train(X_train, y_train, nb_epoch, 1, X_test, y_test)            \n",
    "            \n",
    "#test model\n",
    "accuracy=diva_model.test(X_test, y_test, 1) \n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
