{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 960 (CNMeM is enabled with initial size: 70.0% of memory, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This compares DIVA to MLP with extremely small numbers of hidden units:1 to 10; most MNIST MLP scripts use ~500.\n",
    "DIVA does surprisingly well at very low hidden units, considering that MLP falls apart. \n",
    "I've tested MLP with 1 hidden and ~100 epochs once and it didn't improve much.\n",
    "As DIVA is based on autoencoders, it is plausible that it performs well when \n",
    "dimensionality reduction would otherwise be required, and that appears to be the case here.\n",
    "\n",
    "As an aside, if there is more 'standard' jargon for this sort of test, please notify the author. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import DIVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this cell is taken almost verbatim from the keras examples.\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop \n",
    "from keras.utils import np_utils\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "#categorical encoding for softmax MLPs\n",
    "Y_train_cat = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test_cat = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 hidden units\n",
      "Training MLP\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "nb_epoch = 50\n",
    "input_shape = 784\n",
    "MLP_batch_size=1\n",
    "\n",
    "\n",
    "MLP_accuracy=[]\n",
    "DIVA_accuracy=[]\n",
    "\n",
    "for x in range(1,10):\n",
    "    num_hidden = x\n",
    "    print('Testing %d hidden units'%num_hidden)\n",
    "\n",
    "    mlp = Sequential()\n",
    "    mlp.add(Dense(num_hidden, input_shape=(input_shape,)))\n",
    "    mlp.add(Activation('relu'))\n",
    "    mlp.add(Dense(10))\n",
    "    mlp.add(Activation('softmax'))\n",
    "\n",
    "    mlp.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
    "\n",
    "    print('Training MLP')\n",
    "\n",
    "    history = mlp.fit(X_train, Y_train_cat,\n",
    "                        batch_size=MLP_batch_size, nb_epoch=nb_epoch,\n",
    "                        verbose=0, validation_data=(X_test, Y_test_cat))\n",
    "    score = mlp.evaluate(X_test, Y_test_cat, verbose=0)\n",
    "    \n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    MLP_accuracy.append(score[1])\n",
    "\n",
    "    continue\n",
    "    print('Training DIVA')\n",
    "    #compile model\n",
    "    diva_model = DIVA.diva(nb_classes, input_shape, num_hidden, hidden_act='relu', \n",
    "                           loss='mean_squared_error', optimizer=SGD(), compare=DIVA.compareMAE)\n",
    "\n",
    "    #train model\n",
    "    train_metrics=diva_model.train(X_train, y_train, nb_epoch, 1, X_test, y_test)            \n",
    "\n",
    "    #test model\n",
    "    DIVA_accuracy.append(diva_model.test(X_test, y_test, 1)) \n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: graph loss and accuracy in matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2094,\n",
       " 0.30509999999999998,\n",
       " 0.38109999999999999,\n",
       " 0.74839999999999995,\n",
       " 0.67169999999999996,\n",
       " 0.84389999999999998,\n",
       " 0.90069999999999995,\n",
       " 0.91369999999999996,\n",
       " 0.90380000000000005]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(MLP_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.675, 0.6754, 0.7608, 0.8355, 0.7178, 0.8511, 0.7217, 0.8692, 0.7866]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(DIVA_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
